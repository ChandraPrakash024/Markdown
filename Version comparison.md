# <p align = "center"> Version Comparison b/w v1, v2, v3 </p>

| **Field**                              | **V1 Architecture**                                             | **V2 Architecture**                                          | **V3 Architecture**                                            |
|----------------------------------------|-----------------------------------------------------------------|--------------------------------------------------------------|----------------------------------------------------------------|
| **Ethernet IP**                        | - Uses Xilinx **AXI EthernetLite IP v3.0**.<br> - Handles one TX and one RX buffer, limiting parallelism.<br> - Latency is high as it waits for entire frames to be stored before processing.<br> - Includes unnecessary features like CRC checks and MAC filtering.<br> - No **promiscuous mode** for capturing all packets. | - Still uses **AXI EthernetLite IP**.<br> - Improved usage of dual-port buffers for TX and RX, allowing for higher throughput.<br> - Enables better parallelism, handling more packets concurrently.<br> - Some latency reduction due to partial packet processing but still stores full frame before processing headers.<br> - CRC and MAC filtering remain present, which aren't necessary. | - Uses **custom Ethernet IP**.<br> - Eliminates Xilinx IP overhead (e.g., CRC checks, MAC filtering).<br> - Fully integrates with MPD for online processing.<br> - Processes headers as they are received (on-the-fly).<br> - **Promiscuous mode** implemented for full packet capture.<br> - Reduced latency. |
| **Ethernet Transactor**                | - Retrieves and stores entire Ethernet frames from the IP before forwarding them to the **Master Packet Dealer (MPD)**.<br> - Significant storage duplication as both headers and payloads are stored in registers/flip-flops.<br> - Limited to **one TX/RX buffer**, restricting concurrent processing. | - Transactor starts processing packet data as it is received word-by-word, reducing latency.<br> - Frame is immediately passed to the MPD.<br> - Dual TX/RX buffer support enhances parallelism and throughput. | - Fully integrated into the custom Ethernet IP.<br> - Transactor now allows real-time processing, passing headers to MPD and firewall instantly.<br> - Frame can be transmitted without storing unnecessary payloads.<br> - Maximizes throughput with efficient pipelining. |
| **Master Packet Dealer (MPD)**         | - Receives full frames from the transactor before processing the headers.<br> - Splits header and payload; sends the header to the firewall and stores the payload in the **Packet Reference Table (PRT)**.<br> - Delays in packet forwarding due to waiting for full frame receipt. | - Begins processing packet headers as soon as they arrive (without waiting for the full frame).<br> - Improved parallelism; sends headers to the firewall while payload is still being received.<br> - Reduced latency between reception and processing. | - Fully pipelined: MPD processes headers as they arrive, sending them to the firewall immediately.<br> - On-the-fly packet forwarding, reducing unnecessary delays and optimizing real-time packet classification. |
| **Packet Reference Table (PRT)**       | - Stores payloads in **flip-flops and registers**, leading to high resource consumption.<br> - Payloads are stored before classification, leading to potential wastage if the packet is flagged as unsafe.<br> - Inefficient memory usage when large numbers of packets are received. | - Uses **BRAM slices** to store packet payloads, reducing resource consumption.<br> - Improves memory efficiency while still storing payloads before firewall classification.<br> - More scalable due to the optimized use of FPGA resources. | - Efficiently stores packet payloads in BRAM slices.<br> - On-the-fly processing: unsafe packets are discarded immediately, without payload storage.<br> - Improved resource utilization and faster memory access.<br> - Greater scalability and less wastage of resources. |
| **Firewall**                           | - Uses a **Bloom Filter** for rule checking.<br> - Headers are sent to the firewall only after the full frame is received, adding latency.<br> - If flagged as unsafe, packet headers are passed to the **Shakti CPU** for further analysis.<br> - Classification is sequential, leading to longer wait times. | - The firewall starts classifying packets as soon as headers are received, reducing latency.<br> - Parallelism enables faster classification.<br> - The Bloom filter remains the primary rule-checking mechanism, but with better integration with the MPD.<br> - Lower latency than V1. | - Immediate classification: firewall processes headers in real-time, as soon as they arrive.<br> - **Tight integration** with custom Ethernet IP and MPD for faster decision-making.<br> - Reduced reliance on Shakti CPU due to faster Bloom filter classification.<br> - Highly optimized for high-speed packet classification. |
| **Shakti CPU**                         | - Handles packets flagged as unsafe by the firewall.<br> - Performs **linear search** for rule matching.<br> - Limited parallelism, leading to bottlenecks when many packets are flagged.<br> - Uses the **FIDES scheme** for secure compartmentalization, protecting memory from exploits. | - Fewer packets need deeper inspection due to more efficient firewall classification.<br> - **Linear search** for flagged packets remains, but Shakti CPU is invoked less frequently.<br> - FIDES ensures compartmentalized security for rule checking. | - Even fewer packets require the Shakti CPUâ€™s intervention, due to the efficiency of the Bloom filter and firewall.<br> - Linear search remains for flagged packets but is rarely used.<br> - The FIDES scheme remains critical, ensuring memory safety and security. |
| **Processing Flow**                    | - Ethernet frame is fully received and stored before headers are processed.<br> - Headers are sent to the firewall sequentially.<br> - Unsafe packets are flagged, then analyzed by Shakti CPU.<br> - Payload retrieval and transmission happen after full classification.<br> - High latency. | - Ethernet frames are processed as they are received.<br> - Headers are sent to the firewall while the payload is still being received.<br> - Parallelism reduces latency.<br> - Shakti CPU is used less often due to faster Bloom filter classification.<br> - Transmission of safe packets is faster. | - Real-time, **on-the-fly processing**: headers are classified as soon as they arrive.<br> - No delays in forwarding safe packets.<br> - Unsafe packets are discarded before unnecessary data is stored.<br> - Shakti CPU is rarely needed, making the system highly efficient.<br> - Extremely low latency (213 clock cycles). |
| **Resource Utilization**               | - Very high **LUT** and **flip-flop** usage due to full-frame storage in registers.<br> - Limited scalability.<br> - Uses approximately **15839 Slice LUTs** and **57417 Slice Registers**.<br> - Inefficient use of FPGA resources. | - Reduced resource usage by storing payloads in BRAM.<br> - Improved scalability due to better use of hardware resources.<br> - Uses around **1605 Slice LUTs** and **1696 Slice Registers**.<br> - More efficient but still room for improvement. | - Highly optimized, with minimal LUT and register usage.<br> - Efficient use of BRAM for packet storage.<br> - Uses around **764 Slice LUTs** and **782 Slice Registers**, a **20x reduction** compared to V1.<br> - Scalable and resource-efficient. |
| **Latency**                            | - **10642 clock cycles** for processing a single packet.<br> - High latency due to waiting for the full frame before classification.<br> - Packet storage and retrieval delays increase total processing time. | - Reduced to **10636 clock cycles**.<br> - Latency reduced by processing headers in parallel with payload reception.<br> - Pipelined processing speeds up decision-making. | - Dramatically reduced latency of **213 clock cycles**.<br> - On-the-fly header processing eliminates delays.<br> - Minimal packet classification and transmission times. |
| **Throughput**                         | - Limited throughput due to sequential frame processing and single TX/RX buffer usage.<br> - Around **40-50 Mbps** throughput.<br> - The system cannot handle high-speed traffic efficiently. | - Throughput improved to match **100 Mbps**, the maximum allowed by the MII interface.<br> - Better use of dual TX/RX buffers and parallelism. | - Full utilization of **100 Mbps throughput** due to custom Ethernet IP.<br> - Throughput is no longer limited by Ethernet IP, making the firewall the primary bottleneck. |
| **Scalability**                        | - Poor scalability due to high resource consumption and inefficient memory usage.<br> - Struggles to handle large volumes of traffic. | - More scalable than V1 due to reduced resource consumption and better memory management.<br> - Can handle more traffic but still has limitations with extremely high-speed networks. | - Highly scalable: efficient use of BRAM and minimal resource consumption.<br> - Can handle high-speed traffic efficiently, and optimized for growth. |
| **Challenges Addressed**               | - High latency.<br> - Poor resource utilization.<br> - Limited scalability and high-power consumption.<br> - Inefficient storage and processing.<br> - High workload for the Shakti CPU. | - Reduced resource consumption by using BRAM.<br> - Reduced latency through parallelism and pipelining.<br> - Improved throughput and scalability.<br> - Still relies on Xilinx Ethernet IP, which limits customization. | - Solves the problem of high resource consumption and poor scalability.<br> - Custom Ethernet IP removes unnecessary overhead.<br> - Fully integrated pipeline processing significantly reduces latency.<br


---

### **Summary of Key Differences**:
- **V1**: Basic design with high resource consumption and latency. Limited parallelism and throughput make it inefficient for high-speed networks.
- **V2**: Introduces improvements in resource utilization and latency, using better memory management and parallel processing, but still constrained by the use of Xilinx IP.
- **V3**: A major overhaul, with a custom Ethernet IP, efficient on-the-fly processing, reduced resource consumption, and significantly improved latency and scalability. V3 represents a fully optimized, scalable, and high-performance firewall architecture.
